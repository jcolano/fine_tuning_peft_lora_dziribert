{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using stratified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin c:\\Python310\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary c:\\Python310\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cpu.so...\n",
      "argument of type 'WindowsPath' is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    PeftType,\n",
    "    LoraConfig,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import evaluate\n",
    "import torch\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random \n",
    "from random import seed\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"alger-ia/dziribert_sentiment\"\n",
    "task = \"mrpc\"\n",
    "num_epochs = 20\n",
    "lr = 1e-3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(k in model_name_or_path for k in (\"gpt\", \"opt\", \"bloom\")):\n",
    "    padding_side = \"left\"\n",
    "else:\n",
    "    padding_side = \"right\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side)\n",
    "if getattr(tokenizer, \"pad_token_id\") is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrame to Hugging Face dataset\n",
    "\n",
    "df_train = pd.read_csv('dataset_stratified_train.csv') \n",
    "df_val = pd.read_csv('dataset_stratified_val.csv')\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "validation_dataset = Dataset.from_pandas(df_val)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"glue\", task) \n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return metric.compute(predictions=predictions, references=labels) \n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions[:, 1]  # Get the probabilities of the positive class\n",
    "    threshold = 0.5  # Define your threshold here\n",
    "    predictions = (predictions > threshold).astype(int)  # Apply threshold\n",
    "    return metric.compute(predictions=predictions, references=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # max_length=None => use the model max length (it's actually the default)\n",
    "    outputs = tokenizer(examples[\"text\"], truncation=True, max_length=None)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600f78ea3bf84cb19202bbd320a9e482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8341 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6302f133a40840ae95f24dd6b00cc7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2780 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 8341\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting Query and Value gives the best performance, according to the paper (chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/pdf/2106.09685.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", num_virtual_tokens=20, encoder_hidden_size=128)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\", \n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.35,\n",
    "    bias=\"none\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 594438 || all params: 125035782 || trainable%: 0.47541430980133353\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./dziribert-peft-lora-v3-tuning-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjcolanotoro\u001b[0m (\u001b[33mjcolano\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Dropbox\\Desktop\\MAchineLearning\\OMDENA\\algeria_text_content_classification\\wandb\\run-20230807_152722-yo59irut</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jcolano/huggingface/runs/yo59irut' target=\"_blank\">pious-pond-33</a></strong> to <a href='https://wandb.ai/jcolano/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jcolano/huggingface' target=\"_blank\">https://wandb.ai/jcolano/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jcolano/huggingface/runs/yo59irut' target=\"_blank\">https://wandb.ai/jcolano/huggingface/runs/yo59irut</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6809ba5c513a4704959c2102d5787be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2610 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4299, 'learning_rate': 0.0008084291187739465, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6553d2aa30784edbac743f44baa92d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3707495927810669, 'eval_accuracy': 0.8172661870503597, 'eval_f1': 0.8128224023581428, 'eval_runtime': 139.8814, 'eval_samples_per_second': 19.874, 'eval_steps_per_second': 0.622, 'epoch': 1.0}\n",
      "{'loss': 0.3273, 'learning_rate': 0.0006168582375478927, 'epoch': 1.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f00ff05dd8c47719845f99c74a2fe7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3924570083618164, 'eval_accuracy': 0.8359712230215828, 'eval_f1': 0.828828828828829, 'eval_runtime': 140.7992, 'eval_samples_per_second': 19.744, 'eval_steps_per_second': 0.618, 'epoch': 2.0}\n",
      "{'loss': 0.25, 'learning_rate': 0.00042528735632183906, 'epoch': 2.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e33f58dce0b4517a37c70ae47531b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3767654597759247, 'eval_accuracy': 0.8089928057553957, 'eval_f1': 0.8100178890876565, 'eval_runtime': 139.3729, 'eval_samples_per_second': 19.946, 'eval_steps_per_second': 0.624, 'epoch': 3.0}\n",
      "{'loss': 0.2067, 'learning_rate': 0.00023371647509578544, 'epoch': 3.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e0fd68a5764d92986d30a9d6556054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4279506802558899, 'eval_accuracy': 0.8237410071942446, 'eval_f1': 0.8191881918819188, 'eval_runtime': 147.8405, 'eval_samples_per_second': 18.804, 'eval_steps_per_second': 0.588, 'epoch': 4.0}\n",
      "{'loss': 0.1696, 'learning_rate': 4.21455938697318e-05, 'epoch': 4.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e355f704a2e24c5f8c8050cc97a9f591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5083689093589783, 'eval_accuracy': 0.839568345323741, 'eval_f1': 0.8283294842186297, 'eval_runtime': 141.4707, 'eval_samples_per_second': 19.651, 'eval_steps_per_second': 0.615, 'epoch': 5.0}\n",
      "{'train_runtime': 4354.0169, 'train_samples_per_second': 9.579, 'train_steps_per_second': 0.599, 'train_loss': 0.27082695285022484, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2610, training_loss=0.27082695285022484, metrics={'train_runtime': 4354.0169, 'train_samples_per_second': 9.579, 'train_steps_per_second': 0.599, 'train_loss': 0.27082695285022484, 'epoch': 5.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./dziribert_peft_lora_finetune_v5\\\\tokenizer_config.json',\n",
       " './dziribert_peft_lora_finetune_v5\\\\special_tokens_map.json',\n",
       " './dziribert_peft_lora_finetune_v5\\\\vocab.txt',\n",
       " './dziribert_peft_lora_finetune_v5\\\\added_tokens.json',\n",
       " './dziribert_peft_lora_finetune_v5\\\\tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path=\"./dziribert_peft_lora_finetune_v5\"\n",
    "trainer.model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path where the model and tokenizer were saved\n",
    "model_path = \"./dziribert_peft_lora_finetune_v5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = model_path\n",
    "loaded_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = AutoModelForSequenceClassification.from_pretrained(loaded_config.base_model_name_or_path)\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(loaded_config.base_model_name_or_path)\n",
    "loaded_model = PeftModel.from_pretrained(inference_model, peft_model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with the data3 dataset, the last one annotated by Dihia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the Excel file to CSV\n",
    "# data_xlsx = pd.read_excel('./datasets_other/data3.xlsx')\n",
    "# data_xlsx.to_csv('data_test.csv', index=False)\n",
    "\n",
    "# # Load the new CSV file into a DataFrame\n",
    "# df = pd.read_csv('data_test.csv')\n",
    "\n",
    "# # Add a new column 'label2' with default value as None\n",
    "# df['label2'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('./datasets_annotated/dataset_16k_autolabel.csv')\n",
    "#df = pd.read_csv('data4.csv')\n",
    "df = pd.read_csv(\"./dataset_stratified_test.csv\")\n",
    "\n",
    "df['label2'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2781/2781 [01:26<00:00, 32.00it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Iterate over the DataFrame and update the 'label' column based on the classifier's result\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    try:\n",
    "        inputs = loaded_tokenizer(row['text'], truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = loaded_model(**inputs).logits\n",
    "            #print(outputs)\n",
    "\n",
    "        paraphrased_text = torch.softmax(outputs, dim=1).tolist()[0]\n",
    "\n",
    "        new_label = paraphrased_text.index(max(paraphrased_text))\n",
    "\n",
    "        df.at[index, 'label2'] = new_label\n",
    "\n",
    "        # Add the probability of the positive class to your DataFrame\n",
    "        df.at[index, 'probability_positive'] = paraphrased_text[1]  # Assuming that the positive class is labeled as 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # if index >= 2500:\n",
    "    #     break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./datasets_annotated/dataset_16k_autolabel2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 0: 1472 matches out of 1560\n",
      "Value 1: 897 matches out of 1221\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame and it has columns 'label' and 'label2'\n",
    "matches = df[df['label'] == df['label2']]\n",
    "\n",
    "# Count matches for each value\n",
    "value_0_matches = sum(matches['label'] == 0)\n",
    "value_1_matches = sum(matches['label'] == 1)\n",
    "\n",
    "# Count total 0's and 1's in 'label'\n",
    "total_0 = sum(df['label'] == 0)\n",
    "total_1 = sum(df['label'] == 1)\n",
    "\n",
    "print(f\"Value 0: {value_0_matches} matches out of {total_0}\")\n",
    "print(f\"Value 1: {value_1_matches} matches out of {total_1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8518518518518519\n",
      "Precision: 0.9106598984771573\n",
      "Recall: 0.7346437346437347\n",
      "F1 Score: 0.8132366273798731\n",
      "Confusion Matrix:\n",
      "[[1472   88]\n",
      " [ 324  897]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# Make sure your labels are in numerical format\n",
    "df['label'] = df['label'].astype(np.int64)\n",
    "df['label2'] = df['label2'].astype(np.int64)\n",
    "\n",
    "# Define true and predicted response values\n",
    "y_true = df['label']\n",
    "y_pred = df['label2']\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "precision = metrics.precision_score(y_true, y_pred)\n",
    "recall = metrics.recall_score(y_true, y_pred)\n",
    "f1_score = metrics.f1_score(y_true, y_pred)\n",
    "#auc_roc = metrics.roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# Compute confusion matrix\n",
    "confusion_matrix = metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1_score}')\n",
    "#print(f'AUC-ROC: {auc_roc}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.9415648165648165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = df['label']\n",
    "y_score = df['probability_positive']  # use the probabilities of the positive class\n",
    "\n",
    "auc_roc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "print(f'AUC-ROC: {auc_roc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows where 'label' is 0 and 'label2' is 1\n",
    "selected_rows = df[(df['label'] == 0) & (df['label2'] == 1)]\n",
    "\n",
    "# Save selected rows to a csv file\n",
    "selected_rows.to_csv('data4_false_positives.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
